
services:
  broker:
    build: ./data_broker
    ports: 
      - "3000:3000"

  simulator-a:
    build: ./eeg_acquisition
    environment:
      ACQ_PORT: "13854"
      PACKET_INTERVAL: "1.0"
    command: python simulator.py
    ports: 
      - "13854:13854"

  simulator-b:
    build: ./eeg_acquisition
    environment:
      ACQ_PORT: "13855"
      PACKET_INTERVAL: "1.0"
    command: python simulator.py
    ports:
      - "13855:13855"

  acquisition-a:
    build: ./eeg_acquisition
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      PLAYER_ID: "1"
      ACQ_PORT: "13854"
      BROKER_URL: "http://broker:3000"
      POOR_SIGNAL_LEVEL_THRESHOLD: "0"
      EEG_HOST: "simulator-a"
      # EEG_HOST: "host.docker.internal" 
    command: python acquisition_service.py
    depends_on: 
      - broker
      - simulator-a

  acquisition-b:
    build: ./eeg_acquisition
    environment:
      PLAYER_ID: "2"
      ACQ_PORT: "13855"
      BROKER_URL: "http://broker:3000"
      POOR_SIGNAL_LEVEL_THRESHOLD: "0"
      EEG_HOST: "simulator-b"
      # EEG_HOST: "host.docker.internal"

    command: python acquisition_service.py
    depends_on: 
      - broker
      - simulator-b

  raw_data_collector:
    build: ./data_pipeline/raw_data_collector
    environment:
      BROKER_URL: "http://broker:3000"
      RAW_DATA_PATH: "/data/raw_data" # ATUALIZADO
    volumes:
      # Mapeia a pasta `data` local para a pasta `/data` do container
      - ./data_pipeline/data:/data # ATUALIZADO
    depends_on:
      - broker
    command: sh -c "sleep 5 && python -u collector.py"

  etl_processor:
    build: ./data_pipeline/etl_processor
    environment:
      RAW_DATA_PATH: "/data/raw_data"
      TRUSTED_DATA_PATH: "/data/trusted_data"
    volumes:
      # Compartilha EXATAMENTE O MESMO volume
      - ./data_pipeline/data:/data # ATUALIZADO
    
  refined_processor:
    build: ./data_pipeline/refined_processor
    environment:
      TRUSTED_DATA_PATH: "/data/trusted_data"
      REFINED_DATA_PATH: "/data/refined_data"
    volumes:
      # Compartilha o mesmo volume novamente
      - ./data_pipeline/data:/data
    

  # dashboard:
  #   build: ./dashboard

  #   ports:
  #     - "8000:80"

  #   depends_on:
  #     - broker

# volumes:
#   db_data:
